# 本地運行中文大型語言模型：llama 和 Alpaca 使用指南 📘

在開發者社區引起廣泛關注的大型語言模型（`LLMs`）開闢了無數可能性，特別在聊天機器人和內容創建等領域。現在，通過[llama.cpp](https://github.com/ggerganov/llama.cpp)這個C++庫，我們可以在本地機器上，不依賴外部雲服務，運行中文大型語言模型——`LLaMA`和`Alpaca`。

# LLaMA模型簡介
LLaMA由Meta AI設計，擁有從7億到65億參數的多樣模型，並於2023年2月24日公開發布。儘管其最小版本LLaMA 7B的規模較小，但它仍提供了顯著的語言處理能力。
官方研究論文: [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)

# 🚀 在 llama.cpp 中實現 LLMs 的本地運行

llama.cpp 是一個C++庫，專為在不依賴外部雲服務的情況下，在您自己的本地機器上運行中文大型語言模型——llama 和 Alpaca 而設計。通過這個項目，我們致力於提供一個平台，讓開發者能夠輕鬆地在自己的系統上運行和利用大型語言模型的威力，開創更多的可能性和應用場景。

## 快速入門

這個快速入門指南將引導您在配備Intel CPU的Windows系統上安裝和配置llama.cpp。我們將帶領您從編譯器的安裝到模型的運行，讓您完全感受到llama和Alpaca這兩種中文大語言模型的魅力。

- [前往Windows Intel CPU安裝和配置指南](./Windows_Installation/Guide.md)

# 🛠️ 技術深入：GGML與量化策略

在解密llama和Alpaca中文大型語言模型的運行原理時，GGML和量化策略成為關鍵的技術元素。這兩者不僅最大化了模型的運行效率，也確保了模型在各種硬件配置上的靈活適用。

## GGML：張量庫的核心與運行機制
`GGML`，作為一個C++張量庫，支持在CPU或是搭配GPU的硬體環境下運行LLMs。這一點尤其對於本地運行大型語言模型而言，實現了一個高效且靈活的運行環境。GGML格式的模型可以直接從`Hugging Face Models`庫中下載，這些模型均為開源且免費提供。經過這個張量庫的處理，模型能夠在本地環境中得到有效的讀取和運行，降低了與外部雲服務交互的延遲，提高了模型的反應速度和數據保密性。

## 量化：資源使用的微調和擴展
`量化`是一種關鍵技術，目的在於優化模型的資源使用並擴展其在不同硬體上的運行能力。它涉及降低模型權重的數位精度，以減輕計算負擔和內存使用。GGML支援包括4位、5位和8位在內的多種量化策略，允許開發者在計算效率和模型性能之間進行權衡，確保模型即使在資源有限的情況下也能保持穩定運行。

# 🌱 未來規劃

- 探索並提供對其他加速計算庫的支持和搭建指南。
- 預計將添加對M系列晶片的macOS的安裝教學。

感謝您閱讀，願您的編碼之旅一帆風順！
